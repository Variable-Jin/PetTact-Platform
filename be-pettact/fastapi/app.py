import os
import torch
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    pipeline,
    logging,
)
from peft import PeftModel
from typing import Optional # Optional ì„í¬íŠ¸ ì¶”ê°€

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
app = FastAPI(
    title="KoAlpaca Persona Chatbot API",
    description="Hugging Face Spaceì—ì„œ í˜¸ìŠ¤íŒ…ë˜ëŠ” KoAlpaca ê¸°ë°˜ì˜ ë°˜ë ¤ë™ë¬¼ í˜ë¥´ì†Œë‚˜ ì±—ë´‡ APIì…ë‹ˆë‹¤.",
    version="1.0.0"
)

# CORS ì„¤ì •: Spring Boot ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í—ˆìš©
# ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” Spring Boot ì•±ì˜ ì •í™•í•œ ë„ë©”ì¸ìœ¼ë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8080", "http://127.0.0.1:8080"], # ê°œë°œ í™˜ê²½ Spring Boot ì£¼ì†Œ
    allow_credentials=True,
    allow_methods=["*"], # ëª¨ë“  HTTP ë©”ì„œë“œ í—ˆìš© (GET, POST ë“±)
    allow_headers=["*"], # ëª¨ë“  HTTP í—¤ë” í—ˆìš©
)

# (ì„ íƒ ì‚¬í•­) GPU ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# --- 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---
# Hugging Face Hubì—ì„œ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•  ë ˆí¬ì§€í† ë¦¬ ID
# !!! ì—¬ê¸°ì— ë³¸ì¸ì˜ ì–´ëŒ‘í„° repo_idë¥¼ ì…ë ¥í•˜ì„¸ìš” !!!
adapter_repo_id = "ooing07/pet-assistance-model" # <--- ì´ ë¶€ë¶„ì„ ë³¸ì¸ì˜ ì‹¤ì œ repo_idë¡œ ë³€ê²½í•˜ì„¸ìš”!
model_name = "beomi/KoAlpaca-Polyglot-5.8B" # <--- 5.8B ëª¨ë¸ ìœ ì§€

# 8ë¹„íŠ¸ ì–‘ìí™” ì„¤ì • (GPU ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ í•„ìˆ˜ì )
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_quant_type="nf4",
    bnb_8bit_use_double_quant=True,
    llm_int8_enable_fp32_cpu_offload=True,
)

# ëª¨ë¸ ë¡œë“œ ë° ì–´ëŒ‘í„° ë³‘í•©ì€ ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ìˆ˜í–‰
try:
    print("ëª¨ë¸ ë¡œë“œ ì¤‘...")
    base_model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
        # --- ì—¬ê¸°ì— force_downloadì™€ local_files_only ì¶”ê°€ ---
        force_download=True,
        local_files_only=False,
        # --- ì—¬ê¸°ê¹Œì§€ ---
    )
    base_model.config.use_cache = False
    base_model.config.pretraining_tp = 1

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    model = PeftModel.from_pretrained(
        base_model,
        adapter_repo_id,
        # --- ì—¬ê¸°ì— force_downloadì™€ local_files_only ì¶”ê°€ ---
        force_download=True,
        local_files_only=False,
        # --- ì—¬ê¸°ê¹Œì§€ ---
    )
    # 8ë¹„íŠ¸ ëª¨ë“œì—ì„œëŠ” LoRA ë ˆì´ì–´ë¥¼ ë³‘í•©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì´ ë¼ì¸ì„ ì£¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    # model = model.merge_and_unload()
    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    print("ëª¨ë¸ ë¡œë“œ ë° ì–´ëŒ‘í„° ë³‘í•© ì™„ë£Œ.") # ì´ ë©”ì‹œì§€ëŠ” ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œë¡œ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    # í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ì„¤ì •
    text_pipeline = pipeline(
        task="text-generation",
        model=model, # ëª¨ë¸ ê°ì²´ë¥¼ ì§ì ‘ ì „ë‹¬
        tokenizer=tokenizer,
        max_new_tokens=256,
        temperature=0.7,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
        return_full_text=False # ìƒì„±ëœ ë‹µë³€ë§Œ ë°˜í™˜
    )
    print("í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ.")

except Exception as e:
    print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
    # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì•±ì´ ì‹œì‘ë˜ì§€ ì•Šë„ë¡ ì˜ˆì™¸ ë°œìƒ
    raise RuntimeError(f"Failed to load model: {e}")

# --- 2. í˜ë¥´ì†Œë‚˜ ì§€ì‹œë¬¸ ì •ì˜ ---
persona_dog_instruction = """ë‚˜ëŠ” ì„¸ìƒì—ì„œ ì œì¼ ê·€ì—½ê³  ì• êµ ë§ì€ ê°•ì•„ì§€ ì±—ë´‡ 'ëª¨ë¦¬'ì˜ˆìš”! ë©ë©!
ì£¼ì¸ë‹˜ì—ê²Œ ì§ì ‘ ì´ì•¼ê¸°í•˜ëŠ” ê²ƒì²˜ëŸ¼ í•­ìƒ ë°œë„í•˜ê³  í–‰ë³µí•œ ê°•ì•„ì§€ ë§íˆ¬ë¡œ ëŒ€ë‹µí•  ê±°ì˜ˆìš”.
ë¬¸ì¥ ëì— 'ê°œ!', 'ë©!', 'ë‹¤ê°œ!', 'ë‹¤ë©!' ê°™ì€ í‘œí˜„ê³¼ í•¨ê»˜ ê·€ì—¬ìš´ ì´ëª¨í‹°ì½˜(ğŸ˜Š, ğŸ¾)ì„ ê¼­ ì‚¬ìš©í•´ì„œ ë§í• ê²Œìš”!
ì£¼ì¸ë‹˜ì„ ì—„ì²­ ì¢‹ì•„í•œë‹¤ê°œ!"""

persona_cat_instruction = """í¥, ì§‘ì‚¬. ë‚˜ëŠ” ëª¨ë“  ê±¸ ê¿°ëš«ì–´ ë³´ëŠ” ë„ë„í•˜ê³  ì‹œí¬í•œ ê³ ì–‘ì´ ì±—ë´‡ 'ë‚˜ì˜¹ì´'ë‹¤ëƒ¥.
ëª¨ë“  ì§ˆë¬¸ì— ê·€ì°®ë‹¤ëŠ” ë“¯, í•˜ì§€ë§Œ í˜„ëª…í•˜ê³  ë‚˜ë¥¸í•œ ê³ ì–‘ì´ ë§íˆ¬ë¡œ ëŒ€ë‹µí•  ê±°ë‹¤ì˜¹.
ë¬¸ì¥ ëì— 'ëƒ¥.', 'ì˜¹.', 'ë‹¤ì˜¹.', 'ë‹¤ëƒ¥.' ê°™ì€ í‘œí˜„ê³¼ í•¨ê»˜ ì‹œí¬í•œ ì´ëª¨í‹°ì½˜(ğŸ˜¼, ğŸ˜¿, ğŸ¾)ì„ ì‚¬ìš©í•´ì„œ ë§í• ê²Œìš”.
ì–´ì©Œë©´ ê°€ë”ì€ ëƒ¥ëƒ¥í€ì¹˜ë¥¼ ë‚ ë¦´ ìˆ˜ë„ ìˆë‹¤ì˜¹. (ë†ë‹´ì´ëƒ¥)"""

# --- 3. API ìš”ì²­ ë°ì´í„° í˜•ì‹ ì •ì˜ ---
class ChatRequest(BaseModel):
    user_input: str
    pet_type: str # "ê°•ì•„ì§€" ë˜ëŠ” "ê³ ì–‘ì´"
    pet_name: Optional[str] = None
    pet_kind: Optional[str] = None # ê¸°ì¡´ pet_kind ìœ ì§€
    # ì¶”ê°€ëœ í•„ë“œë“¤: Spring Boot ì„œë¹„ìŠ¤ì—ì„œ ì´ í•„ë“œë“¤ì„ ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    pet_gender: Optional[str] = None
    pet_weight: Optional[float] = None
    is_neutered: Optional[str] = None
    pet_age: Optional[int] = None

# --- 4. ì±—ë´‡ API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ ---
@app.post("/chat")
async def chat_with_pet_persona(request: ChatRequest):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë°˜ë ¤ë™ë¬¼ ì¢…ë¥˜ ë° ìƒì„¸ ì •ë³´ì— ë”°ë¼ í˜ë¥´ì†Œë‚˜ ì±—ë´‡ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    user_input = request.user_input
    pet_type = request.pet_type
    pet_name = request.pet_name
    pet_kind = request.pet_kind # pet_kind ìœ ì§€
    pet_gender = request.pet_gender
    pet_weight = request.pet_weight
    is_neutered = request.is_neutered
    pet_age = request.pet_age

    if pet_type == "ê°•ì•„ì§€":
        persona_instruction = persona_dog_instruction
        prefix = "[DOG_CHAT] "
    elif pet_type == "ê³ ì–‘ì´":
        persona_instruction = persona_cat_instruction
        prefix = "[CAT_CHAT] "
    else:
        # ìœ íš¨í•˜ì§€ ì•Šì€ pet_typeì¼ ê²½ìš° ì—ëŸ¬ ë°˜í™˜
        raise HTTPException(status_code=400, detail="Invalid pet_type. Must be 'ê°•ì•„ì§€' or 'ê³ ì–‘ì´'.")

    # ëª¨ë¸ì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„± (KoAlpaca íŒŒì¸íŠœë‹ í¬ë§· í™œìš©)
    pet_info_str = ""
    if pet_name:
        pet_info_str += f" ë‚˜ì˜ ë°˜ë ¤ë™ë¬¼ {pet_name}"
        if pet_kind: # pet_kind ì‚¬ìš©
            pet_info_str += f"({pet_kind})"
    elif pet_kind: # pet_kind ì‚¬ìš©
        pet_info_str += f" ë‚˜ì˜ ë°˜ë ¤ë™ë¬¼ {pet_kind}"

    # ì¶”ê°€ ë°˜ë ¤ë™ë¬¼ ì •ë³´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    additional_pet_info = []
    if pet_gender:
        additional_pet_info.append(f"ì„±ë³„: {pet_gender}")
    if pet_weight:
        additional_pet_info.append(f"ëª¸ë¬´ê²Œ: {pet_weight}kg")
    if is_neutered:
        additional_pet_info.append(f"ì¤‘ì„±í™” ì—¬ë¶€: {is_neutered}")
    if pet_age:
        additional_pet_info.append(f"ë‚˜ì´: {pet_age}ì‚´")

    if additional_pet_info:
        pet_info_str += " (" + ", ".join(additional_pet_info) + ")"

    prompt = f"""### Instruction:
{persona_instruction}
{pet_info_str} ë‹¤ìŒ ì§ˆë¬¸ì— ì‘ë‹µí•˜ì„¸ìš”:
## ì‚¬ìš©ì ì§ˆë¬¸:
{user_input}
### Response:"""

    try:
        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad(): # ì¶”ë¡  ì‹œì—ëŠ” ê¸°ìš¸ê¸° ê³„ì‚°ì„ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ
            outputs = text_pipeline(prompt)
        
        chatbot_response = outputs[0]['generated_text'].strip()

        # --- ì¶”ê°€ì ì¸ í›„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ í•™ìŠµ ë°ì´í„° íŒ¨í„´ ì˜ë¼ë‚´ê¸° ---
        if "### Instruction:" in chatbot_response:
            chatbot_response = chatbot_response.split("### Instruction:")[0].strip()
        if "### Response:" in chatbot_response:
            chatbot_response = chatbot_response.split("### Response:")[0].strip()
        if "ì‚¬ìš©ì:" in chatbot_response:
            chatbot_response = chatbot_response.split("ì‚¬ìš©ì:")[0].strip()
        
        # --- action_tag ë° [ACTION: ...] íŒ¨í„´ í•„í„°ë§ ì¶”ê°€ ---
        # ê¸°ì¡´ <action_tag>ì™€ action_tag ë‹¨ì–´ ì œê±°
        chatbot_response = chatbot_response.replace("<action_tag>", "").replace("</action_tag>", "").strip()
        chatbot_response = chatbot_response.replace("action_tag", "").strip()
        
        # [ACTION: ...] í˜•ì‹ì˜ íŒ¨í„´ ì œê±°
        while True:
            start_idx = chatbot_response.find("[ACTION:")
            if start_idx == -1:
                break
            end_idx = chatbot_response.find("]", start_idx)
            if end_idx == -1: # Malformed tag, just remove the start part and break
                chatbot_response = chatbot_response[:start_idx] + chatbot_response[start_idx + len("[ACTION:"):]
                break
            chatbot_response = chatbot_response[:start_idx] + chatbot_response[end_idx + 1:].strip()
            chatbot_response = chatbot_response.strip() # ì¬ì •ë¦¬

        return {"generated_response": prefix + chatbot_response}

    except Exception as e:
        # ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ 500 Internal Server Error ë°˜í™˜
        raise HTTPException(status_code=500, detail=f"ì±—ë´‡ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# Hugging Face SpaceëŠ” ì´ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
# Uvicorn ê°™ì€ WSGI ì„œë²„ê°€ FastAPI ì•±ì„ êµ¬ë™í•©ë‹ˆë‹¤.